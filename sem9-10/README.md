# Лабораторная работа: Сравнение производительности последовательных и параллельных алгоритмов

## Описание работы
Цель данной лабораторной работы заключалась в исследовании производительности последовательных и параллельных алгоритмов решения систем уравнений методом Гаусса-Зейделя. Были реализованы три варианта алгоритмов:

1. **Последовательный алгоритм**.
2. **Параллельный алгоритм с использованием OpenMP**.
3. **Параллельный алгоритм с использованием MPI**.

Мы измерили время выполнения алгоритмов для различных размеров сетки и рассчитали ускорение параллельных алгоритмов относительно последовательного.

## Реализованные шаги

### 1. Реализация алгоритмов
- **Последовательный алгоритм**: реализован на C++ без использования технологий параллелизации.
- **OpenMP**: добавлена поддержка многопоточности с использованием переменной `OMP_NUM_THREADS` для управления числом потоков.
- **MPI**: алгоритм был разделён между несколькими процессами с обменом граничными строками данных.

### 2. Сбор данных
Для всех трёх алгоритмов измерено время выполнения для сеток следующих размеров: `10x10`, `100x100`, `1000x1000`, `2000x2000`, `3000x3000`. В OpenMP и MPI тестировались различные количества потоков/процессов (2, 4, 6).

### 3. Построение таблиц
- Таблица для **MPI** включает время выполнения и ускорение для 2, 4 и 6 процессов.
- Таблица для **OpenMP** включает время выполнения и ускорение для 2, 4 и 6 потоков.

### 4. Построение графиков
- Графики времени выполнения:
  - Последовательный алгоритм против OpenMP.
  - Последовательный алгоритм против MPI.
- Графики ускорения для OpenMP и MPI.

### 5. Вывод таблиц и графиков
Результаты сохранены:
- Таблицы результатов в Excel-файл `results_comparison.xlsx` (с отдельными листами для MPI и OpenMP).
- Графики:
  - `mpi_execution_time_comparison.png` (сравнение времени выполнения для MPI).
  - `openmp_execution_time_comparison.png` (сравнение времени выполнения для OpenMP).

## Выводы

1. **MPI**:
   - Ускорение увеличивается с числом процессов, но заметно снижается при увеличении размера сетки из-за накладных расходов на коммуникацию между процессами.
   - Лучшие результаты достигаются для небольших размеров сетки и малого числа процессов.

2. **OpenMP**:
   - Ускорение близко к линейному при увеличении числа потоков для небольших размеров сетки.
   - Для больших сеток накладные расходы синхронизации ограничивают эффективность.

3. **Общая производительность**:
   - Последовательный алгоритм демонстрирует ожидаемо низкую производительность для больших сеток.
   - OpenMP эффективен для использования в среде с разделяемой памятью.
   - MPI более полезен для кластеров, но требует оптимизации при обработке крупных данных.

## Файлы и результаты
- `results_comparison.xlsx`: Excel-файл с таблицами для OpenMP и MPI.
- `mpi_execution_time_comparison.png`: график времени выполнения для MPI.
- `openmp_execution_time_comparison.png`: график времени выполнения для OpenMP.

## Рекомендации
Для больших систем уравнений целесообразно использовать MPI с оптимизацией коммуникации или гибридный подход (MPI + OpenMP) для увеличения производительности.

